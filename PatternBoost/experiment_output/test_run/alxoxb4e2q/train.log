INFO - 11/13/25 16:01:56 - 0:00:00 - ============ Initialized logger ============
INFO - 11/13/25 16:01:56 - 0:00:00 - batch_size: 32
                                     command: python fc_loop.py --cpu true --sample-only 2000 --max-steps 1000 --max_epochs 3 --n_tokens 10 --dump_path experiment_output --exp_name test_run --exp_id "alxoxb4e2q"
                                     cpu: True
                                     debug: False
                                     debug_slurm: False
                                     dump_path: experiment_output/test_run/alxoxb4e2q
                                     exp_id: alxoxb4e2q
                                     exp_name: test_run
                                     final_database_size: 50000
                                     gen_batch_size: 1000
                                     global_rank: 0
                                     is_master: True
                                     is_slurm_job: False
                                     learning_rate: 0.0005
                                     local_rank: 0
                                     master_port: -1
                                     max_epochs: 3
                                     max_output_length: 160
                                     max_steps: 1000
                                     multi_gpu: False
                                     multi_node: False
                                     n_embd: 64
                                     n_embd2: 32
                                     n_gpu_per_node: 1
                                     n_head: 8
                                     n_layer: 4
                                     n_nodes: 1
                                     n_tokens: 10
                                     nb_local_searches: 1200
                                     nb_threads: 1
                                     node_id: 0
                                     num_initial_empty_objects: 50000
                                     num_workers: 8
                                     sample_only: 2000
                                     seed: -1
                                     target_db_size: 50000
                                     temperature: 1.0
                                     top_k: -1
                                     type: transformer
                                     weight_decay: 0.01
                                     world_size: 1
INFO - 11/13/25 16:01:56 - 0:00:00 - The experiment will be stored in experiment_output/test_run/alxoxb4e2q
                                     
INFO - 11/13/25 16:01:56 - 0:00:00 - Running command: python fc_loop.py --cpu true --sample-only 2000 --max-steps 1000 --max_epochs 3 --n_tokens 10 --dump_path experiment_output --exp_name test_run

INFO - 11/13/25 16:01:56 - 0:00:00 - seed: 664437480
INFO - 11/13/25 16:01:56 - 0:00:00 - JULIA_NUM_THREADS is set to 1
INFO - 11/13/25 16:08:49 - 0:06:53 - Converting to tokenized format...
INFO - 11/13/25 16:08:49 - 0:06:53 - 0 / 50000
INFO - 11/13/25 16:08:49 - 0:06:53 - 10000 / 50000
INFO - 11/13/25 16:08:49 - 0:06:53 - 20000 / 50000
INFO - 11/13/25 16:08:50 - 0:06:54 - 30000 / 50000
INFO - 11/13/25 16:08:50 - 0:06:54 - 40000 / 50000
INFO - 11/13/25 16:08:50 - 0:06:54 - initializing at generation: 1
INFO - 11/13/25 16:08:52 - 0:06:56 - number of examples in the dataset: 50000
INFO - 11/13/25 16:08:52 - 0:06:56 - max word length: 90
INFO - 11/13/25 16:08:52 - 0:06:56 - number of unique characters in the vocabulary: 2
INFO - 11/13/25 16:08:52 - 0:06:56 - vocabulary:
INFO - 11/13/25 16:08:52 - 0:06:56 - ['V0', 'V1']
INFO - 11/13/25 16:08:52 - 0:06:56 - split up the dataset into 49000 training examples and 1000 test examples
INFO - 11/13/25 16:08:52 - 0:06:56 - dataset determined that: vocab_size=11, block_size=161
INFO - 11/13/25 16:08:52 - 0:06:56 - model #params: 211776
INFO - 11/13/25 16:08:52 - 0:06:56 - ============ Start of generation 1 ============
INFO - 11/13/25 16:08:52 - 0:06:56 - Memory allocated:  0.00MB, reserved: 0.00MB
INFO - 11/13/25 16:08:52 - 0:06:56 - training
INFO - 11/13/25 16:08:57 - 0:07:01 - step 0 | loss 2.8446 | step time 1245.97ms
INFO - 11/13/25 16:09:20 - 0:07:24 - step 100 | loss 0.7032 | step time 200.92ms
INFO - 11/13/25 16:09:45 - 0:07:49 - step 200 | loss 0.6907 | step time 222.24ms
INFO - 11/13/25 16:10:07 - 0:08:11 - step 300 | loss 0.6887 | step time 232.15ms
INFO - 11/13/25 16:10:33 - 0:08:37 - step 400 | loss 0.6870 | step time 263.36ms
INFO - 11/13/25 16:11:02 - 0:09:06 - step 500 | loss 0.6868 | step time 690.75ms
INFO - 11/13/25 16:11:20 - 0:09:24 - step 500 train loss: 0.6866514682769775 test loss: 0.6866967082023621
INFO - 11/13/25 16:11:20 - 0:09:24 - test loss 0.6866967082023621 is the best so far, saving model to experiment_output/test_run/alxoxb4e2q/model.pt
INFO - 11/13/25 16:13:37 - 0:11:41 - step 600 | loss 0.6860 | step time 218.14ms
INFO - 11/13/25 16:13:59 - 0:12:03 - step 700 | loss 0.6858 | step time 187.99ms
INFO - 11/13/25 16:14:22 - 0:12:26 - step 800 | loss 0.6858 | step time 202.65ms
INFO - 11/13/25 16:14:48 - 0:12:52 - step 900 | loss 0.6856 | step time 256.14ms
INFO - 11/13/25 16:15:09 - 0:13:13 - Memory allocated:  0.00MB, reserved: 0.00MB
INFO - 11/13/25 16:15:09 - 0:13:13 - generating
